

<!DOCTYPE html>
<html lang="zh-CN" data-default-color-scheme=auto>



<head>
  <meta charset="UTF-8">
  <link rel="apple-touch-icon" sizes="76x76" href="/img/jg-square.png">
  <link rel="icon" href="/img/jg.png">
  <meta name="viewport" content="width=device-width, initial-scale=1.0, maximum-scale=5.0, shrink-to-fit=no">
  <meta http-equiv="x-ua-compatible" content="ie=edge">
  
  <meta name="theme-color" content="#2f4154">
  <meta name="author" content="jin">
  <meta name="keywords" content="">
  
    <meta name="description" content="序列标注任务与 Sequence to Sequence 的神经网络.">
<meta property="og:type" content="article">
<meta property="og:title" content="自然语言处理基础 | 3 序列任务">
<meta property="og:url" content="https://disembo.github.io/Note/Course/fnlp/3/index.html">
<meta property="og:site_name" content="Jin&#39;s Blog">
<meta property="og:description" content="序列标注任务与 Sequence to Sequence 的神经网络.">
<meta property="og:locale" content="zh_CN">
<meta property="og:image" content="https://raw.githubusercontent.com/Disembo/blog-media/master/course/fnlp/hmm.png">
<meta property="og:image" content="https://raw.githubusercontent.com/Disembo/blog-media/master/course/fnlp/seq2seq-rnn.png">
<meta property="og:image" content="https://raw.githubusercontent.com/Disembo/blog-media/master/course/fnlp/seq2seq-attn.png">
<meta property="og:image" content="https://raw.githubusercontent.com/Disembo/blog-media/master/course/fnlp/seq2seq-self-attn.png">
<meta property="article:published_time" content="2025-03-20T04:00:00.000Z">
<meta property="article:modified_time" content="2025-09-15T03:03:01.810Z">
<meta property="article:author" content="jin">
<meta property="article:tag" content="自然语言处理">
<meta name="twitter:card" content="summary_large_image">
<meta name="twitter:image" content="https://raw.githubusercontent.com/Disembo/blog-media/master/course/fnlp/hmm.png">
  
  
    <meta name="referrer" content="no-referrer-when-downgrade">
  
  
  <title>自然语言处理基础 | 3 序列任务 | Jin&#39;s Blog</title>

  <link  rel="stylesheet" href="https://lib.baomitu.com/twitter-bootstrap/4.6.1/css/bootstrap.min.css" />



  <link  rel="stylesheet" href="https://lib.baomitu.com/github-markdown-css/4.0.0/github-markdown.min.css" />

  <link  rel="stylesheet" href="https://lib.baomitu.com/hint.css/2.7.0/hint.min.css" />

  <link  rel="stylesheet" href="https://lib.baomitu.com/fancybox/3.5.7/jquery.fancybox.min.css" />



<!-- 主题依赖的图标库，不要自行修改 -->
<!-- Do not modify the link that theme dependent icons -->

<link rel="stylesheet" href="//at.alicdn.com/t/font_1749284_hj8rtnfg7um.css">



<link rel="stylesheet" href="//at.alicdn.com/t/font_1736178_lbnruvf0jn.css">


<link  rel="stylesheet" href="/css/main.css" />


  <link id="highlight-css" rel="stylesheet" href="/css/highlight.css" />
  
    <link id="highlight-css-dark" rel="stylesheet" href="/css/highlight-dark.css" />
  



  
<link rel="stylesheet" href="/css/additional.css">
<link rel="stylesheet" href="/css/html-hint.css">



  <script id="fluid-configs">
    var Fluid = window.Fluid || {};
    Fluid.ctx = Object.assign({}, Fluid.ctx)
    var CONFIG = {"hostname":"disembo.github.io","root":"/","version":"1.9.7","typing":{"enable":true,"typeSpeed":70,"cursorChar":"_","loop":false,"scope":[]},"anchorjs":{"enable":true,"element":"h1,h2,h3,h4,h5,h6","placement":"left","visible":"hover","icon":"#"},"progressbar":{"enable":true,"height_px":3,"color":"#29d","options":{"showSpinner":false,"trickleSpeed":100}},"code_language":{"enable":true,"default":"TEXT"},"copy_btn":true,"image_caption":{"enable":true},"image_zoom":{"enable":true,"img_url_replace":["",""]},"toc":{"enable":true,"placement":"right","headingSelector":"h1,h2,h3,h4,h5,h6","collapseDepth":3},"lazyload":{"enable":true,"loading_img":"/img/loading.gif","onlypost":false,"offset_factor":2},"web_analytics":{"enable":false,"follow_dnt":true,"baidu":null,"google":{"measurement_id":null},"tencent":{"sid":null,"cid":null},"woyaola":null,"cnzz":null,"leancloud":{"app_id":null,"app_key":null,"server_url":null,"path":"window.location.pathname","ignore_local":false}},"search_path":"/local-search.xml","include_content_in_search":true};

    if (CONFIG.web_analytics.follow_dnt) {
      var dntVal = navigator.doNotTrack || window.doNotTrack || navigator.msDoNotTrack;
      Fluid.ctx.dnt = dntVal && (dntVal.startsWith('1') || dntVal.startsWith('yes') || dntVal.startsWith('on'));
    }
  </script>
  <script  src="/js/utils.js" ></script>
  <script  src="/js/color-schema.js" ></script>
  


  
<!-- hexo injector head_end start --><script src="/js/mathjax-cfg.js"></script><script async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-svg.js"></script><!-- hexo injector head_end end --><meta name="generator" content="Hexo 5.4.2"></head>


<body>
  

  <header>
    

<div class="header-inner" style="height: 70vh;">
  <nav id="navbar" class="navbar fixed-top  navbar-expand-lg navbar-dark scrolling-navbar">
  <div class="container">
    <a class="navbar-brand" href="/">
      <strong>Jin&#39;s Blog</strong>
    </a>

    <button id="navbar-toggler-btn" class="navbar-toggler" type="button" data-toggle="collapse"
            data-target="#navbarSupportedContent"
            aria-controls="navbarSupportedContent" aria-expanded="false" aria-label="Toggle navigation">
      <div class="animated-icon"><span></span><span></span><span></span></div>
    </button>

    <!-- Collapsible content -->
    <div class="collapse navbar-collapse" id="navbarSupportedContent">
      <ul class="navbar-nav ml-auto text-center">
        
          
          
          
          
            <li class="nav-item">
              <a class="nav-link" href="/" target="_self">
                <i class="iconfont icon-home-fill"></i>
                <span>Home</span>
              </a>
            </li>
          
        
          
          
          
          
            <li class="nav-item">
              <a class="nav-link" href="/archives/" target="_self">
                <i class="iconfont icon-archive-fill"></i>
                <span>Archives</span>
              </a>
            </li>
          
        
          
          
          
          
            <li class="nav-item">
              <a class="nav-link" href="/categories/" target="_self">
                <i class="iconfont icon-category-fill"></i>
                <span>Categories</span>
              </a>
            </li>
          
        
          
          
          
          
            <li class="nav-item">
              <a class="nav-link" href="/tags/" target="_self">
                <i class="iconfont icon-tags-fill"></i>
                <span>Tags</span>
              </a>
            </li>
          
        
          
          
          
          
            <li class="nav-item">
              <a class="nav-link" href="/about/" target="_self">
                <i class="iconfont icon-user-fill"></i>
                <span>About</span>
              </a>
            </li>
          
        
          
          
          
          
            <li class="nav-item">
              <a class="nav-link" href="/links/" target="_self">
                <i class="iconfont icon-link-fill"></i>
                <span>Links</span>
              </a>
            </li>
          
        
        
          <li class="nav-item" id="search-btn">
            <a class="nav-link" target="_self" href="javascript:;" data-toggle="modal" data-target="#modalSearch" aria-label="Search">
              <i class="iconfont icon-search"></i>
            </a>
          </li>
          
        
        
          <li class="nav-item" id="color-toggle-btn">
            <a class="nav-link" target="_self" href="javascript:;" aria-label="Color Toggle">
              <i class="iconfont icon-dark" id="color-toggle-icon"></i>
            </a>
          </li>
        
      </ul>
    </div>
  </div>
</nav>

  

<div id="banner" class="banner" parallax=true
     style="background: url('/img/default.png') no-repeat center center; background-size: cover;">
  <div class="full-bg-img">
    <div class="mask flex-center" style="background-color: rgba(0, 0, 0, 0.3)">
      <div class="banner-text text-center fade-in-up">
        <div class="h2">
          
            <span id="subtitle" data-typed-text="自然语言处理基础 | 3 序列任务"></span>
          
        </div>

        
          
  <div class="mt-3">
    
    
      <span class="post-meta">
        <i class="iconfont icon-date-fill" aria-hidden="true"></i>
        <time datetime="2025-03-20 12:00" pubdate>
          2025年3月20日 中午
        </time>
      </span>
    
  </div>

  <div class="mt-1">
    

    

    
    
  </div>


        
      </div>

      
    </div>
  </div>
</div>

</div>

  </header>

  <main>
    
      

<div class="container-fluid nopadding-x">
  <div class="row nomargin-x">
    <div class="side-col d-none d-lg-block col-lg-2">
      

    </div>

    <div class="col-lg-8 nopadding-x-md">
      <div class="container nopadding-x-md" id="board-ctn">
        <div id="board">
          <article class="post-content mx-auto">
            <h1 id="seo-header">自然语言处理基础 | 3 序列任务</h1>
            
            
              <div class="markdown-body">
                
                <html><head></head><body>
<h2 id="sequence-tagging">4  Sequence Tagging</h2>
<p>给句子中的每个词贴标签, 例如</p>
<ul>
<li>词性标注</li>
<li>分词 (标记 B, I)</li>
<li>NP Chunking: find basic noun phrases from text.
<ul>
<li>B: beginning of a NP</li>
<li>I: continuing NP</li>
<li>O: other words</li>
</ul></li>
<li>Named Entity Recognition: find named entities from text, e.g., names
of persons, locations, organizations, etc.</li>
</ul>
<p>一大困难是歧义性 ambiguity.</p>
<h3 id="hidden-markov-models">4.1  Hidden Markov Models</h3>
<h4 id="assumptions">4.1.1  Assumptions</h4>
<p>设长度为 <span class="math inline">\(n\)</span> 的句子 <span class="math inline">\((x_1,\dots,x_n)\)</span>, 每个单词的标签 (POS,
part-of-speech) 为 <span class="math inline">\((y_1,\dots,y_n)\)</span>.
则我们关心的是在 <span class="math inline">\(x_i\)</span> 条件下, <span class="math inline">\(y_i\)</span> 概率的最大值: <span class="math display">\[
\argmax_{(y_1,\dots,y_n)} p(y_1,\dots,y_n\mid x_1,\dots,x_n).
\]</span> 这等价于求解 <span class="math display">\[
\argmax_{(y_1,\dots,y_n)}\;
\underbrace{p(y_1,\dots,y_n)}_{\sf LM}\;
p(x_1,\dots,x_n\mid y_1,\dots,y_n).
\]</span> 注意到第一项其实就是一个 language model, 可以用 bigram 或者
trigram 表示, 而对于第二项, 我们做出如下假设: (conditional independent
assumption) <span class="math display">\[
p(x_1,\dots,x_n\mid y_1,\dots,y_n)=\prod_{i=1}^np(x_i\mid y_i),
\]</span> 就得到了 <strong>hidden Markov model (HMM)</strong>.
模型参数包括:</p>
<ul>
<li>(transition probabilities) 由标签得到下一个标签, 即 bigram 或者
trigram 的参数 <span class="math inline">\(p(y_n\mid y_{n-1})\)</span>
或 <span class="math inline">\(p(y_n\mid y_{n-2},y_{n-1})\)</span>.</li>
<li>(emmision probabilities) 由标签得到单词, <span class="math inline">\(p(x_n\mid y_n)\)</span>.</li>
</ul>
<p>它的运作原理:</p>
<p><img src="https://raw.githubusercontent.com/Disembo/blog-media/master/course/fnlp/hmm.png" cloud-img="" style="zoom:35%;" lazyload="true"></p>
<ul>
<li>初始状态 <span class="math inline">\(y_1\sim P_{\sf
start}(Y)\)</span>.</li>
<li>根据 <span class="math inline">\(y_1\)</span> 得到 <span class="math inline">\(x_1\sim P_{\sf emmision}(X\mid y_1)\)</span>
(emmision).</li>
<li>根据 <span class="math inline">\(y_1\)</span> 得到 <span class="math inline">\(y_2\sim P_{\sf transition}(Y\mid y_1)\)</span>
(transition).</li>
</ul>
<p>以此类推, 直到结束.</p>
<h4 id="parameter-estimation">4.1.2  Parameter estimation</h4>
<p>类似于 n-gram, 通过 MLE 进行参数估计.</p>
<h4 id="decoding">4.1.3  Decoding</h4>
<p>给了一个训练好的 HMM 模型 (包括所有参数), 以及一个句子 <span class="math inline">\((x_1,\dots,x_n)\)</span>, 我们如何找到最优的 POS
序列 <span class="math inline">\((y_1,\dots,y_n)\)</span> 使得其
transition probs (这里用 trigram) 和 emmision probs 的乘积最大?</p>
<p>暴力搜索肯定不行, 我们考虑动态规划, 即 <strong>Viterbi 算法</strong>.
该算法维护一个动态规划表 <span class="math inline">\(\pi(k,u,v)\)</span>, 表示 the maximum probability
of any tag sequences ending with <span class="math inline">\(u,v\)</span> at position <span class="math inline">\(k\)</span>, 即 <span class="math display">\[
\pi(k,u,v)
:= \max_{\substack{y_{k-1}=u\\y_k=v}} \pqty{
  \prod_{i=1}^k p(y_i\mid y_{i-2},y_{i-1}) \cdot
  \prod_{i=1}^k p(x_i\mid y_i)
  }.
\]</span> 记第 <span class="math inline">\(k\)</span> 位置的所有可能的
POS 集合为 <span class="math inline">\(S_k\)</span>. 在第 <span class="math inline">\(0\)</span> 位置为 <span class="math inline">\({\sf
START}\)</span>, 第 <span class="math inline">\(n+1\)</span> 位置为
<span class="math inline">\({\sf STOP}\)</span>.</p>
<ol type="1">
<li><p>初始化 <span class="math inline">\(\pi(0,{\sf START},{\sf
START})=1\)</span>.</p></li>
<li><p>对所有 <span class="math inline">\(k\in\{1,2,\dots,n\}\)</span>
以及 <span class="math inline">\(u\in S_{k-1}\)</span> 和 <span class="math inline">\(v\in S_k\)</span>,</p>
<ul>
<li><p>状态转移: <span class="math display">\[
\pi(k,u,v)
= \max_{w\in S_{k-2}} \pi(k-1,w,u)
  \cdot p(v\mid w,u)
  \cdot p(x_i\mid v).
\]</span></p></li>
<li><p>记录路径: <span class="math display">\[
{\sf path}(k,u,v)
= \argmax_{w\in S_{k-2}} \pi(k-1,w,u)
  \cdot p(v\mid w,u)
  \cdot p(x_i\mid v).
\]</span></p></li>
</ul></li>
<li><p>复原路径 (POS):</p>
<ul>
<li>枚举计算 <span class="math inline">\(y_n,y_{n-1}\)</span>.</li>
<li>迭代 <span class="math inline">\(y_{n-2}={\sf
path}(n,y_{n-1},y_n)\)</span> 等等.</li>
</ul></li>
</ol>
<p>计算复杂度: <span class="math inline">\(O(n|S|^3)\)</span>.</p>
<h3 id="perceptron-models">4.2  Perceptron models</h3>
<div class="note note-secondary">
<p>A classifier for sequence tagging. 与 HMM 不同, classifier
为每个单词分别预测其 POS (individual decisions), 而 HMM 是 sequence of
decisions.</p>
</div>
<h4 id="features">4.2.1  Features</h4>
<p>回顾 NLP 中的 feature, 这是一些关于 <span class="math inline">\((x,y)\)</span> 的 0/1 函数. 如果我们要 tag 这句话
<em>I love Beijing</em> 中的 <em>Beijing</em>,
则我们可以构造一些关于该单词的 clues: <span class="math display">\[
\Align{
&amp;\verb|cur_Beijing|,&amp;
&amp;\verb|pre1_love|,&amp;
&amp;\verb|pref_Be|,&amp;
&amp;\verb|cap_1|,&amp;\dots
}
\]</span> 把 clue 与 labels (NNP, VB...) 结合, 就得到了 feature: <span class="math display">\[
\Align{
&amp;\orange{\verb|cur_Beijing_NNP|},&amp;
&amp;\orange{\verb|pre1_love_NNP|},&amp;
&amp;\orange{\verb|pref_Be_NNP|},&amp;
&amp;\orange{\verb|cap_1_NNP|},&amp;\dots \\
&amp;\blue{\verb|cur_Beijing_VB|},&amp;
&amp;\blue{\verb|pre1_love_VB|},&amp;
&amp;\blue{\verb|pref_Be_VB|},&amp;
&amp;\blue{\verb|cap_1_VB|},&amp;\dots \\
&amp;\dots&amp;
&amp;\dots&amp;
&amp;\dots&amp;
&amp;\dots&amp;\dots
}
\]</span> 每个 feature 的值为 0/1. 我们构建一个线性分类器, 为每个
feature 配一个权重 <span class="math inline">\(\lambda\)</span>, 有
<span class="math display">\[
\Align{
&amp;\orange{\lambda_\verb|cur_Beijing_NNP|},&amp;
&amp;\orange{\lambda_\verb|pre1_love_NNP|},&amp;
&amp;\orange{\lambda_\verb|pref_Be_NNP|},&amp;
&amp;\orange{\lambda_\verb|cap_1_NNP|},&amp;\dots \\
&amp;\blue{\lambda_\verb|cur_Beijing_VB|},&amp;
&amp;\blue{\lambda_\verb|pre1_love_VB|},&amp;
&amp;\blue{\lambda_\verb|pref_Be_VB|},&amp;
&amp;\blue{\lambda_\verb|cap_1_VB|},&amp;\dots \\
&amp;\dots&amp;
&amp;\dots&amp;
&amp;\dots&amp;
&amp;\dots&amp;\dots
}
\]</span> 用优化算法求得 <span class="math inline">\(\lambda\)</span>s
之后, 计算该句话中 <em>Beijing</em> 每个类别的得分: <span class="math display">\[
\Align{
{\sf score}({\sf Beijing},\orange{\sf NNP})
&amp;= \orange{\verb|cur_Beijing_NNP|}\cdot
   \orange{\lambda_\verb|cur_Beijing_NNP|} + \cdots \\
{\sf score}({\sf Beijing},\blue{\sf VB})
&amp;= \blue{\verb|cur_Beijing_VB|}\cdot
   \blue{\lambda_\verb|cur_Beijing_VB|} + \cdots \\
\dots\hspace{3em}
}
\]</span> 最终取得分最大的那一类作为预测值.</p>
<h4 id="the-perceptron-alg.">4.2.2  The perceptron alg.</h4>
<p><strong>感知机算法 (the perceptron algorithm)</strong> 是优化 <span class="math inline">\(\lambda\)</span>s 的一种算法.</p>
<ul>
<li><p>输入: 训练集 <span class="math inline">\(\{(x_k,y_k)\}\)</span>,
其中 <span class="math inline">\(x_k\)</span> 是单词, <span class="math inline">\(y_k\)</span> 为 POS label.</p></li>
<li><p>初始化 <span class="math inline">\(\lambda\)</span>s
为零.</p></li>
<li><p>重复执行 <span class="math inline">\(T\)</span> 次:</p>
<ul>
<li>对每个训练数据 <span class="math inline">\((x_k,y_k)\)</span>:
<ul>
<li>计算预测值 <span class="math inline">\(z\leftarrow\argmax_{y\in{\sf
GEN}(x_k)}\sum_i\lambda_{f_i(x_k,y)} f_i(x_k,y)\)</span>.</li>
</ul>
<div class="note note-secondary">
<p>函数 <span class="math inline">\({\sf GEN}(x)\)</span> 给出了单词
<span class="math inline">\(x\)</span> 所有可能的标签的集合.</p>
</div>
<ul>
<li>更新参数: 如果 <span class="math inline">\(z\neq y_k\)</span>
(分类错误), 则:
<ul>
<li><span class="math inline">\(\lambda\leftarrow\lambda+f(x_k,y_k)-f(x_k,z)\)</span>.</li>
</ul></li>
</ul>
<div class="note note-secondary">
<p>惩罚导致分类错误的权重 (减一)、奖励使得分类正确的权重 (加一).</p>
</div></li>
</ul></li>
<li><p>输出 <span class="math inline">\(\lambda\)</span>s.</p></li>
</ul>
<h4 id="the-structured-perceptron-alg.">4.2.3  The structured perceptron
alg.</h4>
<p>The perceptron algorithm 是纯粹局部的 (local), 将每个单词分开处理.
我们希望算法可以考虑一个句子中的历史 labels,
这需要我们将一个句子当成一个整体来预测 (global).</p>
<p>首先引入一些 global 的 features, 比如 <span class="math display">\[
\Align{
&amp;\pink{\verb|prev_N_cur_N|},&amp;
&amp;\pink{\verb|prev2_DT_N_cur_N|},&amp;\dots
}
\]</span> 即 "前一个词标签为 N, 预测当前也为 N" 和 "前两个词标签为 DT,
N, 预测当前为 N" 等等.</p>
<p><strong>结构感知机算法 (the structured perceptron algorithm)</strong>
流程如下:</p>
<ul>
<li><p>输入: 句子集 <span class="math inline">\(S\)</span>, 单词集 <span class="math inline">\(\{(x_k,y_k)\}\)</span>, 其中 <span class="math inline">\(x_k\)</span> 是单词, <span class="math inline">\(y_k\)</span> 为 POS label.</p></li>
<li><p>初始化 <span class="math inline">\(\lambda\)</span>s
为零.</p></li>
<li><p>重复执行 <span class="math inline">\(T\)</span> 次:</p>
<ul>
<li><p>对每句话 <span class="math inline">\(s\in S\)</span>, 每个单词
<span class="math inline">\(x\in s\)</span> 和可能的标签 <span class="math inline">\(y\in{\sf GEN}(x)\)</span>:</p>
<ul>
<li>计算得分 <span class="math inline">\({\sf
score}[x,y]\leftarrow\sum_i\lambda_{f_i(x,y)}f_i(x,y)\)</span>.</li>
</ul>
<div class="note note-secondary">
<p>Score table 是一个 <span class="math inline">\(|s|\times|{\sf
GEN}(x)|\)</span> 大小的矩阵 / lattice.</p>
</div>
<ul>
<li>用 Viterbi 算法解码出 <span class="math inline">\(s\)</span> 的最佳
POS 序列 <span class="math inline">\(y\)</span>s.</li>
</ul>
<div class="note note-secondary">
<p>由于每个单词的 score 与前一个单词的 label 的预测值有关 (因为有 global
features), 所以不能分别计算 score, 而是需要从头开始,
用动态规划算法计算出一个最优 score 序列.</p>
</div>
<ul>
<li>比较 <span class="math inline">\(y\)</span>s 和 gold-standard 序列
<span class="math inline">\(y^*\)</span>s, 更新 <span class="math inline">\(\lambda\)</span>s.</li>
</ul></li>
</ul></li>
</ul>
<h4 id="beam-search">4.2.4  Beam search</h4>
<p>Score table 大小可能非常大, 用 Viterbi 算法效率比较低. <strong>束搜索
(beam search)</strong> 是一种近似算法. 比如说, <span class="math inline">\({\sf GEN}(x)\)</span> 一共有 <span class="math inline">\(200\)</span> 个元素, 我们觉得它太大了, 于是可以在
DP 时只保留 <span class="math inline">\(64\)</span> 个得分最高的类别,
即在每次状态转移时只考虑 <span class="math inline">\(64\times200\)</span> 种组合, 然后取得分最大的
<span class="math inline">\(64\)</span> 个保存下来.</p>
<ul>
<li>Early stop.</li>
<li>CRF.</li>
</ul>
<h2 id="seq2seq-nns">5  Seq2seq NNs</h2>
<p>序列 <span class="math inline">\(\to\)</span> 序列的任务, 如序列翻译,
序列标注.</p>
<h3 id="rnns">5.1  RNNs</h3>
<p>Encoder-Decoder 架构. 将输入序列编码为一个固定长度向量, 再输入
decoder, 自回归生成翻译结果. RNN 采用 LSTM.</p>
<p><img src="https://raw.githubusercontent.com/Disembo/blog-media/master/course/fnlp/seq2seq-rnn.png" cloud-img="" style="zoom:35%;" lazyload="true"></p>
<p>数学模型. 给定输入序列 <span class="math inline">\((x_1,\dots,x_T)\)</span> (如中文句子),
目标是生成翻译后的句子 <span class="math inline">\((y_1,\dots,y_{T'})\)</span> (如英语句子),
建模为 <span class="math display">\[
p(y_1,\dots,y_{T'}) = \prod_{t=1}^{T'} p(y_t\mid
v,y_1,\dots,y_{t-1}),
\]</span> 其中 <span class="math inline">\(v\)</span> 是 LSTM 给出的
<span class="math inline">\((x_1,\dots,x_T)\)</span> 的向量表示.</p>
<p>问题是不能明确捕获两个单词 (如 “爱” 和 ”love“) 间的明确对应.</p>
<h3 id="transformers">5.2  Transformers</h3>
<h4 id="the-attention-mechanism">5.2.1  The attention mechanism</h4>
<p>在 decoder 中, 显式地考虑每个输入单词对当前单词的影响.</p>
<ul>
<li>Query vector <span class="math inline">\(q\)</span>: decoder
的隐状态.</li>
<li>Key vector <span class="math inline">\(k\)</span>: 所有 encoder
的隐状态.</li>
</ul>
<p><img src="https://raw.githubusercontent.com/Disembo/blog-media/master/course/fnlp/seq2seq-attn.png" cloud-img="" style="zoom:35%;" lazyload="true"></p>
<p>不同的计算方式:</p>
<ul>
<li><p>MLP: <span class="math display">\[
\operatorname{attn}(q,k) = w_2\T\tanh(W[q,k]).
\]</span> 当 <span class="math inline">\(q,k\)</span>
长度较小时表现最佳.</p></li>
<li><p>Bilinear: <span class="math display">\[
\operatorname{attn}(q,k) = q\T Wk.
\]</span></p></li>
<li><p>Dot product: <span class="math display">\[
\operatorname{attn}(q,k) = q\T k.
\]</span></p></li>
<li><p>Scaled dot product: <span class="math display">\[
\operatorname{attn}(q,k) = \frac{q\T k}{\sqrt{d}},
\]</span> 其中 <span class="math inline">\(q,k\in\R^d\)</span>.</p></li>
</ul>
<h4 id="self-attention">5.2.2  Self-attention</h4>
<p>上面的 attention 是 encoder 与 decoder 间的联系,
我们也可以建立统一个序列中单词间的联系 (build contextual representation
of a word by integrating information from surrounding words).</p>
<p>对于一个序列 <span class="math inline">\(X=(x_1\T,\dots,x_n\T)\T\)</span> (原始 embedding),
通过三个线性层 <span class="math inline">\(W^Q,W^K,W^V\)</span> 分别得到
query, key, value: <span class="math display">\[
Q=XW^Q, \qquad K=XW^K, \qquad V=XW^V.
\]</span> 应用 scaled dot-product attention, <span class="math display">\[
\operatorname{attn}(Q,K,V)
= \operatorname{softmax}\!\left(\frac{1}{\sqrt{d}} Q\T K \right) V.
\]</span> Multi-head attention 是若干并行的 scaled dot product attention
的拼接, 再投影回原本的维度: <span class="math display">\[
\operatorname{mha}(Q,K,V)
= \left[
    \bigoplus_{i=1}^h
    \operatorname{attn}(QU^Q_i,KU^K_i,VU^V_i)
\right] U^O,
\]</span> 其中 <span class="math inline">\(U^Q_i,U^K_i,U^V_i\)</span>
是每个 head 之前额外的线性层, <span class="math inline">\(U^O\)</span>
是最后的投影层.</p>
<p><img src="https://raw.githubusercontent.com/Disembo/blog-media/master/course/fnlp/seq2seq-self-attn.png" cloud-img="" style="zoom:40%;" lazyload="true"></p>
<h4 id="additional-tricks">5.2.3  Additional tricks</h4>
<p>Attention tricks:</p>
<ul>
<li>Self attention: Each layer combines words with others</li>
<li>Multi-headed attention: 8 attention heads learned independently</li>
<li>Scaled Dot-product attention: Remove bias in dot product when using
large networks</li>
<li>Positional Encodings: Attention 是排列不变的, 需要加入位置编码
<ul>
<li>Make sure that even if we do not have RNN, can still distinguish
positions</li>
<li>区分 “狗咬人” 和 “人咬狗”</li>
</ul></li>
</ul>
<p>Training Tricks</p>
<ul>
<li>Layer normalization: Help ensure that layers remain in reasonable
range
<ul>
<li>Operate on each token level, <span class="math inline">\(\mu=0\)</span>, <span class="math inline">\(\sigma=1\)</span>.</li>
</ul></li>
<li>Specialized training schedule: Adjust default learning rate of the
Adam optimizer</li>
<li>Label smoothing: Insert some uncertainty in the training
process</li>
<li>Masking for efficient training
<ul>
<li>We want to perform training in as few operations as possible using
big matrix multiplies</li>
<li>Causal mask</li>
</ul></li>
</ul>
</body></html>
                
              </div>
            
            <hr/>
            <div>
              <div class="post-metas my-3">
  
    <div class="post-meta mr-3 d-flex align-items-center">
      <i class="iconfont icon-category"></i>
      

<span class="category-chains">
  
  
    
      <span class="category-chain">
        
  <a href="/categories/Note/" class="category-chain-item">Note</a>
  
  
    <span>></span>
    
  <a href="/categories/Note/Course/" class="category-chain-item">Course</a>
  
  

  

      </span>
    
  
</span>

    </div>
  
  
    <div class="post-meta">
      <i class="iconfont icon-tags"></i>
      
        <a href="/tags/%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86/" class="print-no-link">#自然语言处理</a>
      
    </div>
  
</div>


              
  

  <div class="license-box my-3">
    <div class="license-title">
      <div>自然语言处理基础 | 3 序列任务</div>
      <div>https://disembo.github.io/Note/Course/fnlp/3/</div>
    </div>
    <div class="license-meta">
      
        <div class="license-meta-item">
          <div>作者</div>
          <div>jin</div>
        </div>
      
      
        <div class="license-meta-item license-meta-date">
          <div>发布于</div>
          <div>2025年3月20日</div>
        </div>
      
      
      
        <div class="license-meta-item">
          <div>许可协议</div>
          <div>
            
              
              
                <a class="print-no-link" target="_blank" href="https://creativecommons.org/licenses/by-sa/4.0/">
                  <span class="hint--top hint--rounded" aria-label="BY - 署名">
                    <i class="iconfont icon-by"></i>
                  </span>
                </a>
              
                <a class="print-no-link" target="_blank" href="https://creativecommons.org/licenses/by-sa/4.0/">
                  <span class="hint--top hint--rounded" aria-label="SA - 相同方式共享">
                    <i class="iconfont icon-sa"></i>
                  </span>
                </a>
              
            
          </div>
        </div>
      
    </div>
    <div class="license-icon iconfont"></div>
  </div>



              
                <div class="post-prevnext my-3">
                  <article class="post-prev col-6">
                    
                    
                      <a href="/Note/CG/proc-4-parameterization/" title="参数化与纹理">
                        <i class="iconfont icon-arrowleft"></i>
                        <span class="hidden-mobile">参数化与纹理</span>
                        <span class="visible-mobile">上一篇</span>
                      </a>
                    
                  </article>
                  <article class="post-next col-6">
                    
                    
                      <a href="/Note/CG/simu-2-rigid/" title="刚体模拟">
                        <span class="hidden-mobile">刚体模拟</span>
                        <span class="visible-mobile">下一篇</span>
                        <i class="iconfont icon-arrowright"></i>
                      </a>
                    
                  </article>
                </div>
              
            </div>

            
  
  
    <article id="comments" lazyload>
      
  <script type="text/javascript">
    Fluid.utils.loadComments('#comments', function() {
      var light = 'github-light';
      var dark = 'github-dark';
      var schema = document.documentElement.getAttribute('data-user-color-scheme');
      if (schema === 'dark') {
        schema = dark;
      } else {
        schema = light;
      }
      window.UtterancesThemeLight = light;
      window.UtterancesThemeDark = dark;
      var s = document.createElement('script');
      s.setAttribute('src', 'https://utteranc.es/client.js');
      s.setAttribute('repo', 'Disembo/blog-comments');
      s.setAttribute('issue-term', 'pathname');
      
      s.setAttribute('label', 'utterances');
      
      s.setAttribute('theme', schema);
      s.setAttribute('crossorigin', 'anonymous');
      document.getElementById('comments').appendChild(s);
    })
  </script>
  <noscript>Please enable JavaScript to view the comments</noscript>


    </article>
  


          </article>
        </div>
      </div>
    </div>

    <div class="side-col d-none d-lg-block col-lg-2">
      
  <aside class="sidebar" style="margin-left: -1rem">
    <div id="toc">
  <p class="toc-header">
    <i class="iconfont icon-list"></i>
    <span>目录</span>
  </p>
  <div class="toc-body" id="toc-body"></div>
</div>



  </aside>


    </div>
  </div>
</div>





  



  



  



  



  







    

    
      <a id="scroll-top-button" aria-label="TOP" href="#" role="button">
        <i class="iconfont icon-arrowup" aria-hidden="true"></i>
      </a>
    

    
      <div class="modal fade" id="modalSearch" tabindex="-1" role="dialog" aria-labelledby="ModalLabel"
     aria-hidden="true">
  <div class="modal-dialog modal-dialog-scrollable modal-lg" role="document">
    <div class="modal-content">
      <div class="modal-header text-center">
        <h4 class="modal-title w-100 font-weight-bold">搜索</h4>
        <button type="button" id="local-search-close" class="close" data-dismiss="modal" aria-label="Close">
          <span aria-hidden="true">&times;</span>
        </button>
      </div>
      <div class="modal-body mx-3">
        <div class="md-form mb-5">
          <input type="text" id="local-search-input" class="form-control validate">
          <label data-error="x" data-success="v" for="local-search-input">关键词</label>
        </div>
        <div class="list-group" id="local-search-result"></div>
      </div>
    </div>
  </div>
</div>

    

    
  </main>

  <footer>
    <div class="footer-inner">
  
    <div class="footer-content">
       <a href="https://hexo.io" target="_blank" rel="nofollow noopener"><span>Hexo</span></a> <i class="iconfont icon-love"></i> <a href="https://github.com/fluid-dev/hexo-theme-fluid" target="_blank" rel="nofollow noopener"><span>Fluid</span></a> 
    </div>
  
  
    <div class="statistics">
  
  

  
    
      <span id="busuanzi_container_site_pv" style="display: none">
        Views: 
        <span id="busuanzi_value_site_pv"></span>
        
      </span>
    
    
      <span id="busuanzi_container_site_uv" style="display: none">
        Visitors: 
        <span id="busuanzi_value_site_uv"></span>
        
      </span>
    
    
  
</div>

  
  
  
</div>

  </footer>

  <!-- Scripts -->
  
  <script  src="https://lib.baomitu.com/nprogress/0.2.0/nprogress.min.js" ></script>
  <link  rel="stylesheet" href="https://lib.baomitu.com/nprogress/0.2.0/nprogress.min.css" />

  <script>
    NProgress.configure({"showSpinner":false,"trickleSpeed":100})
    NProgress.start()
    window.addEventListener('load', function() {
      NProgress.done();
    })
  </script>


<script  src="https://lib.baomitu.com/jquery/3.6.4/jquery.min.js" ></script>
<script  src="https://lib.baomitu.com/twitter-bootstrap/4.6.1/js/bootstrap.min.js" ></script>
<script  src="/js/events.js" ></script>
<script  src="/js/plugins.js" ></script>


  <script  src="https://lib.baomitu.com/typed.js/2.0.12/typed.min.js" ></script>
  <script>
    (function (window, document) {
      var typing = Fluid.plugins.typing;
      var subtitle = document.getElementById('subtitle');
      if (!subtitle || !typing) {
        return;
      }
      var text = subtitle.getAttribute('data-typed-text');
      
        typing(text);
      
    })(window, document);
  </script>




  
    <script  src="/js/img-lazyload.js" ></script>
  




  
<script>
  Fluid.utils.createScript('https://lib.baomitu.com/tocbot/4.20.1/tocbot.min.js', function() {
    var toc = jQuery('#toc');
    if (toc.length === 0 || !window.tocbot) { return; }
    var boardCtn = jQuery('#board-ctn');
    var boardTop = boardCtn.offset().top;

    window.tocbot.init(Object.assign({
      tocSelector     : '#toc-body',
      contentSelector : '.markdown-body',
      linkClass       : 'tocbot-link',
      activeLinkClass : 'tocbot-active-link',
      listClass       : 'tocbot-list',
      isCollapsedClass: 'tocbot-is-collapsed',
      collapsibleClass: 'tocbot-is-collapsible',
      scrollSmooth    : true,
      includeTitleTags: true,
      headingsOffset  : -boardTop,
    }, CONFIG.toc));
    if (toc.find('.toc-list-item').length > 0) {
      toc.css('visibility', 'visible');
    }

    Fluid.events.registerRefreshCallback(function() {
      if ('tocbot' in window) {
        tocbot.refresh();
        var toc = jQuery('#toc');
        if (toc.length === 0 || !tocbot) {
          return;
        }
        if (toc.find('.toc-list-item').length > 0) {
          toc.css('visibility', 'visible');
        }
      }
    });
  });
</script>


  <script src=https://lib.baomitu.com/clipboard.js/2.0.11/clipboard.min.js></script>

  <script>Fluid.plugins.codeWidget();</script>


  
<script>
  Fluid.utils.createScript('https://lib.baomitu.com/anchor-js/4.3.1/anchor.min.js', function() {
    window.anchors.options = {
      placement: CONFIG.anchorjs.placement,
      visible  : CONFIG.anchorjs.visible
    };
    if (CONFIG.anchorjs.icon) {
      window.anchors.options.icon = CONFIG.anchorjs.icon;
    }
    var el = (CONFIG.anchorjs.element || 'h1,h2,h3,h4,h5,h6').split(',');
    var res = [];
    for (var item of el) {
      res.push('.markdown-body > ' + item.trim());
    }
    if (CONFIG.anchorjs.placement === 'left') {
      window.anchors.options.class = 'anchorjs-link-left';
    }
    window.anchors.add(res.join(', '));

    Fluid.events.registerRefreshCallback(function() {
      if ('anchors' in window) {
        anchors.removeAll();
        var el = (CONFIG.anchorjs.element || 'h1,h2,h3,h4,h5,h6').split(',');
        var res = [];
        for (var item of el) {
          res.push('.markdown-body > ' + item.trim());
        }
        if (CONFIG.anchorjs.placement === 'left') {
          anchors.options.class = 'anchorjs-link-left';
        }
        anchors.add(res.join(', '));
      }
    });
  });
</script>


  
<script>
  Fluid.utils.createScript('https://lib.baomitu.com/fancybox/3.5.7/jquery.fancybox.min.js', function() {
    Fluid.plugins.fancyBox();
  });
</script>


  <script>Fluid.plugins.imageCaption();</script>

  <script  src="/js/local-search.js" ></script>

  <script defer src="https://busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js" ></script>





<!-- 主题的启动项，将它保持在最底部 -->
<!-- the boot of the theme, keep it at the bottom -->
<script  src="/js/boot.js" ></script>


  

  <noscript>
    <div class="noscript-warning">博客在允许 JavaScript 运行的环境下浏览效果更佳</div>
  </noscript>
</body>
</html>
